---
layout: post
title:  "[ML] HunKim`s ML 8"
date:   2017-07-14 01:38:33 +0900
categories: ML
tags: [AI클럽, gitbook, ML, Machine Learning, Deep Learning]

---

# ReLU, Weight 초기화, Dropout, 앙상블
---

![ReLU](http://farm4.static.flickr.com/3434/3942758168_94dac63ff6_z.jpg)

이제 Sigmoid가 아닌 **ReLU** 를 알아보자.

만약 앞서 XOR를 layer를 9단까지 하면 어떻게 될까.

실제로 돌려보면 정확도가 반으로 떨어진다.

왜 그럴까?

그게 바로 `Backpropagation` 때문이다.

2단, 3단정도의 그래프는 잘 학습이 되는데, 많이 늘어나면 정확도가 떨어진다.

그 이유는 `chain rule`로 설명이 되는데, 이전 노드에 항상 1보다 작은 값이 곱해지고 할수록 0에 가까워지므로, 0에 수렴하게 되면서, 결국 입력이 별로 상관이 없어지게 된다.

이 문제 때문에 AI의 2차 winter가 온 것이기도 하다.

그 결과 잘못된 `non-linearity`라고 밝혀졌고, sigmoid가 항상 1보다 작은게 문제라고 나왔다.

따라서 **ReLU(Rectified Linear Unit)** 를 쓰게 되었다.

그러나, 마지막은 sigmoid를 쓰는데, 그 이유는 0~1사이의 값을 주기 위해서다.

그리고 ReLU 말고도 `Leaky ReLU`, `Maxout`, `ELU`등도 있다.

그럼 이제, Weight 초기화를 진행해보자.

항상 Random으로 할 때 보면, 상황에 따라 결과가 다르니, 최적화된 Weight를 알아보는 것이다.

예를 들어, 만약 초기값을 모두 0으로 한다면 GradientDescent는 0이 곱해지며 사라지게 된다.

따라서, 모든 Weight는 0이 되면 안된다.

`DBN`이라는 것을 쓰는데, `Pre-Training`이 필요하다.

여기선 X값만 필요하다.

2개의 layer를 가지고 서로서로 왔다갔다하며 비슷해질 떄까지 학습시킨다.
그리고 그다음 layer... 쭉 해서 마지막 까지 간다. 그때 남아있는 weight가 있을 것이다.

그게 바로 초기화된 weight이다.

근데 이건 조금 옛날 방식이다.

`Xavier/He initialization` 이란 것들이 있다.

`Xavier`방식은 input(fan_in)과 output(fan_out) 수를 이용하는데, weight을 random하게 주는데,

`(fan_in, fan_out)/np.sqrt(fan_in))`을 한 값 사이에서 주는 것이다.

`He`방법은 `(fan_in, fan_out)/np.sqrt(fan_in/2)`를 한 값 사이에서 선택하는 것이다.

그럼 마지막으로 **Dropout** 과 **앙상블** 에 대해 알아보자.

Overfitting이 될 때, `Regularization strength` 라는 방법이 있었다.

이 이외에 **Dropout** 이라는 걸 쓴다.

그만둔다는 뜻인데, 학습 시킨 node를 몇 개 빼는 것이다.

단, 주의할 점은 학습할 때만 그렇게 한다는 것이다.

그럼 **Ensemble(앙상블)** 이 무엇인지 알아보자.

독립적으로 NN을 만들었을 때, Training set을 여러개 만들어서 학습시킬 텐데,

같은 형태로 DeepLearning 모델을 여러개 만든다. 그런데 초기값이 다르니 조금씩 다를것이고 이걸 합쳐 답을 내는 것이다.
