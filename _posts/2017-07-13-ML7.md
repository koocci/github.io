---
layout: post
title:  "[ML] HunKim`s ML 7"
date:   2017-07-13 22:44:33 +0900
categories: ML
tags: [AI클럽, gitbook, ML, Machine Learning, Deep Learning]

---

# XOR, tensorboard
---

![XOR](http://farm7.static.flickr.com/6183/6105259625_91241cb3a7_z.jpg)

모두를 위한 딥러닝 강좌를 듣다보면 AI의 전체적인 스토리를 알 수 있다.

그리고 `XOR`에 대한 문제가 나오는데 이번엔 한번 구현해 볼까한다.

    import tensorflow as tf
    import numpy as np

    x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)
    y_data =np.array([[0], [1], [1], [0]], dtype=np.float32)

    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)
    W = tf.Variable(tf.random_normal([2,1]), name='weight')
    b = tf.Variable(tf.random_normal([1]), name='bias')

    hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for step in range(10001):
            sess.run(train, feed_dict={X: x_data, Y: y_data})
            if step % 100 == 0:
                print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))

        h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
        print("\nHypothesis: ", h, "\nCorrect: ", c, "\nAccuracy: ", a)

위 결과가 어떤가? W를 한층만 헀더니, 정확도가 절반밖에 되지 않는다. 심지어 test dataset을 따로 만들지 않았는데도 말이다.

그래서 다음과 같이 바꾼다.

    import tensorflow as tf
    import numpy as np

    x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)
    y_data =np.array([[0], [1], [1], [0]], dtype=np.float32)

    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)

    W1 = tf.Variable(tf.random_normal([2,2]), name='weight')
    b1 = tf.Variable(tf.random_normal([2]), name='bias')
    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)

    W2 = tf.Variable(tf.random_normal([2,1]), name='weight2')
    b2 = tf.Variable(tf.random_normal([1]), name='bias2')
    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)

    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

    with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for step in range(10001):
        sess.run(train, feed_dict={X: x_data, Y: y_data})
        if step % 100 == 0:
            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))

    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
    print("\nHypothesis: ", h, "\nCorrect: ", c, "\nAccuracy: ", a)

이렇게 layer를 하나 추가하면 1.0으로 정확도가 나온다.

그럼 이제 `TensorBoard`를 알아볼 것이다.

**TensorBoard** 는 우리가 print해서 보는게 매우 구식이라, 이를 그래프로 보여준다.

이를 사용하려면 5가지 방법이 필요하다.

1. 어떤 tensor를 log로 보여줄지 정한다.
2. 모든 summary merge한다.
3. writer와 add graph를 만든다.
4. summary merge와 add_summary를 실행한다.
5. TensorBoard를 실행한다.

첫번째는 다음과 같다.

    w2_hist = tf.summary.histogram("weight2", W2)
    cost_summ = tf.summary.scalar('cost', cost)

데이터에 따라 `histogram과` `scalar라는` 함수를 준다.

`scalar`, `histogram` 은, 값이 하나라면 scalar, 여러개면 histogram에 넣는다.

    summary = tf.summary.merge_all()

두번째는 모두 summary를 하는 작업이다.

    writer = tf.summary.FileWriter('./logs')
    writer.add_graph(sess.graph)

세번째는 파일의 위치를 정하고 그래프를 넣어달라고 하는 것이다.

    s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)
    writer.add_summary(s, global_step=global_step)

summary 자체도 tensor라 sess.run에 넣어준다.
그리고 실제로 파일에 넣게, s를 넣어준다.

    tensorboard --logdir=./logs

마지막으로 실행한다.

그래프를 보고 싶을 땐, 보통 그래프를 한꺼번에 보려면 힘드니, tf.name_scope라는 걸 이용해 지역별로 정할 수 있다.

원래 마지막으로 실행을 하면, 6006번 포트로 되는데,

`ssh -L local_port:127.0.0.1:remote_port username@server.com`

로, 리모트 서버에 `remote_port`를 정하고 포트포워딩을 하면 local에서 서버에 있는 것을 볼 수 있다.

그리고 가끔 같은 형태의 소스를 값을 조금만 변경해 보고 싶을때, 비교해 보고싶을때 방법이 있다.

`tensorboard --logdir = ./logs/xor_logs1`

`tensorboard --logdir = ./logs/xor_logs2`

위 처럼 같은 디렉토리에 넣어두고,

`tensorboard -logdir=./logs`

확인하면 비교가 된다.

혹시나, tensorboard 명령어를 못찾는다면 `python3 -m tensorflow.tensorboard --logdir=./logs/xor_logs_r0_01`로 실행해 보도록 하자.

이후는 다음에 하도록 하겠다.
